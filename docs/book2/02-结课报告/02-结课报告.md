## 研究对象：ORB-SLAM2

### **1）简介：**

`ORB-SLAM` 是西班牙 Zaragoza 大学的 Raúl Mur-Arta 编写的视觉 `SLAM(simultaneous localization and mapping，即时定位与地图构建)` 系统。 它是一个完整的 SLAM 系统，包括视觉里程计、跟踪、回环检测，是一种完全基于稀疏特征点的单目 SLAM 系统，而`ORB-SLAM2`提供了双目立体相机以及`RGBD`相机的接口。其核心是使用 ORB (Orinted FAST and BRIEF) 作为整个视觉 SLAM 中的核心特征。



### **2）功能：**

它能够实时计算并跟踪摄像机在空间中的`运动轨迹`。

也能用于小到桌子，大到多个城市街道的多种规模的场景的`三维重建`。

除此以外，它还具备较好的稳健性，当摄像头跟丢后还能够实时进行全局的重定位找回摄像头位置。

### **3）系统框架：**

![img](https://pic3.zhimg.com/80/v2-b0b335c323b2ff6cd34024f97bbfe266_1440w.webp)

ORB-SLAM 它是由三大块、三个流程同时运行的。第一块是跟踪，第二块是建图，第三块是闭环检测。

1. 跟踪（Tracking）

这一部分主要工作是从图像中提取 ORB 特征，根据上一帧进行姿态估计，或者进行通过全局重定位初始化位姿，然后跟踪已经重建的局部地图，优化位姿，再根据一些规则确定新关键帧。

2. 建图（LocalMapping）

这一部分主要完成局部地图构建。包括对关键帧的插入，验证最近生成的地图点并进行筛选，然后生成新的地图点，使用局部捆集调整（Local BA），最后再对插入的关键帧进行筛选，去除多余的关键帧。

3. 闭环检测（LoopClosing）

这一部分主要分为两个过程，分别是闭环探测和闭环校正。闭环检测先使用 WOB 进行探测，然后通过 Sim3 算法计算相似变换。闭环校正，主要是闭环融合和 Essential Graph 的图优化。

:::note

**Bag-of-words**：词袋算法 ，它是主要用来判断同一个地点是不是被重新访问过，它的算式在实现的原理上可以认为是对每一帧或者叫每一幅图用了很多单词来进行描述。

在实际应用中在这个词袋中使用的单词量会非常大，并且这些词在现实中并没有一个非常明确的物理含义；词袋方案现在主流所采取的方案之一，它做得最好的一方案叫 DBOW2，也是一个开源的方案。

:::

:::note

**Essential Graph**：基于位置优化的实时闭环控制。它通过生成树构建，生成树由系统、闭环控制链接和视图内容关联强边缘进行维护。

:::

### **4）ORB-SLAM2：**

ORB-SLAM2 在 ORB-SLAM 的基础上，还支持标定后的双目相机和 RGB-D 相机。双目对于精度和鲁棒性都会有一定的提升。ORB-SLAM2 是基于单目，双目和 RGB-D 相机的一套完整的 SLAM 方案。它能够实现地图重用，回环检测和重新定位的功能。无论是在室内的小型手持设备，还是到工厂环境的无人机和城市里驾驶的汽车，ORB-SLAM2 都能够在标准的 CPU 上进行实时工作。ORB-SLAM2 在后端上采用的是基于单目和双目的光束法平差优化（BA）的方式，这个方法允许米制比例尺的轨迹精确度评估。此外，ORB-SLAM2 包含一个轻量级的定位模式，该模式能够在允许零点漂移的条件下，利用视觉里程计来追踪未建图的区域并且匹配特征点。

:::note

**Bundle Adjustment**（**BA**）：光束平差法，就是利用非线性最小二乘法来求取相机位姿，三维点坐标。在仅给定相机内部矩阵的条件下，对四周物体进行高精度重建。

Bundle Adjustment 可以将所观测的图像位置和预测的图像位置点进行最小 error 的映射（匹配），由很多非线性函数的平方和表示（error）。因此，最小化 error 由非线性最小二乘法实现。

:::



### **5）工作原理：**

`第一步`，系统读取摄像头参数与`ORB`字典的数据，并且初始化整个系统。

`第二步`，利用ORB算法初始化相机位姿，在完成第一帧图像的特征点的提取后，紧接着可以进行两相邻帧图像的特征点匹配结果。

`第三步`，在得到特征点匹配的信息的基础上，利用双目立体视觉中的三角法，恢复匹配上的特征点的深度，这里我们采用的是单目相机，单目相机在不同位置采集的两张图像，也可以采用类似三角化方法恢复空间点深度。

并同时得到特征点的三维空间位置，此坐标成为世界坐标系，它以初始化时的相机光心为坐标系原点，此外还有相机坐标系、像素坐标系的概念。

`第四步`，初始化得到特征点的空间位置之后，会先将一部分符合要求的特征点标记为路标点（`Mappoint`）,再对路标点利用PnP算法，即给定物体3D点集与对应的图像2D点集，之后进行姿态计算，求解出最终的初始化时对应的相机的位姿；对应的相机位姿可以由四元数表示。

`第五步`，初始化得到了一开始的相机位姿后，不断的重复上述的匹配，跟踪，三角化，计算出每一帧图像对应的摄像机位姿，然后再计算出每两帧图像之间相机的运动变换矩阵（包括旋转矩阵与平移矩阵两部分），然后再利用图优化的方法对估算出的结果做优化，得到更加精确和稳健的结果，最终得到相机的运动轨迹。

简化过程：

[初始化系统]--->[提取每相邻两帧图像的特征点，进行匹配]--->[得到三维点的深度和位置]--->[姿态计算，得到初始化的相机位姿]--->[重复上述操作，得到两帧之间的运动变换矩阵]--->[图优化得到最终相机的运动轨迹]

![在这里插入图片描述](https://img-blog.csdnimg.cn/20190718112608217.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY1MjgyOQ==,size_16,color_FFFFFF,t_70)

### **6）优缺点：**

**优点：**

- 一个代码构造优秀的视觉 SLAM 系统，非常适合移植到实际项目。
- 采用 g2o 作为后端优化工具，能有效地减少对特征点位置和自身位姿的估计误差。
- 采用 DBOW 减少了寻找特征的计算量，同时回环匹配和重定位效果较好。重定位：比如当机器人遇到一些意外情况之后，它的数据流突然被打断了，在 ORB-SLAM 算法下，可以在短时间内重新把机器人在地图中定位。
- 使用了类似「适者生存」的方案来进行关键帧的删选，提高系统追踪的鲁棒性和系统的可持续运行。
- 提供最著名的公共数据集（ KITTI 和 TUM 数据集）的详尽实验结果，以显示其性能。
- 可以使用开源代码，并且还支持使用 ROS。 (Github: [slightech/MYNT-EYE-ORB-SLAM2-Sample](https://link.zhihu.com/?target=https%3A//github.com/slightech/MYNT-EYE-ORB-SLAM2-Sample)）



**缺点：**

- 构建出的地图是稀疏点云图。只保留了图像中特征点的一部分作为关键点，固定在空间中进行定位，很难描绘地图中的障碍物的存在。
- 初始化时最好保持低速运动，对准特征和几何纹理丰富的物体。
- 旋转时比较容易丢帧，特别是对于纯旋转，对噪声敏感，不具备尺度不变性。
- 如果使用纯视觉 slam 用于机器人导航，可能会精度不高，或者产生累积误差，漂移，尽管可以使用 DBoW 词袋来回环检测。最好使用 VSLAM+IMU 进行融合，可以提高精度上去，适用于实际应用中机器人的导航。

### **7）总结：**

简单的说，这个系统的核心思想是，通过图像中跟踪匹配到的特征点的变化反推出相机的位置以及运动，并且用优化库对计算结果进行了优化，还增加了一些回环检测的模块加强系统的稳健性。这和人类在野外通过周围的景物确定自己的位置变化的思路很相似，都是先选定参照物，然后根据观察到的参照物的视野变化估计出自己的运动轨迹。